#meeting #inperson #london  #ai #event
-  met charlie cowan there - rememeber to follow up
- ask for demo playbook


Analysis: Difference Between the Two Generated Answers

After investigating the codebase, I found that there are two different AI generation systems producing these answers:

1. Automatic Answer (Shorter, First One)

- Generated by: expert-conversation Edge Function
- Triggered: Automatically during conversation flow
- Purpose: Provides quick conversational answers to
- Characteristics: guide assessment

- Uses limited document context (1500 chars per doc)
- Part of natural conversation flow
- Focuses on essential information
- Temperature: 0.7 (more creative)
- Uses "we/our" language naturally

2. "Generate from Documents" Answer (Longer, Second One)

- Generated by: extract-answer Edge Function
- Triggered: By clicking "Generate from Documents" button
- Purpose: Comprehensive document-based answer extraction
- Characteristics:
- Uses full document text (up to 30,000 chars)
- More thorough and detailed
- Includes specific data, percentages, dates
- Temperature: 0.3 (more focused/deterministic)
- Strictly evidence-based from documents

Key Technical Differences:

1. Document Context:
- Expert conversation: 1,500 chars per document
- Extract answer: Full document text (30,000 char limit)
2. System Prompts:
- Expert conversation: Focuses on natural conversation and progression
- Extract answer: Strict document extraction with relevance checking
3. Intent:
- Expert conversation: Guide user through assessment questions
- Extract answer: Extract comprehensive answers from project documents

Recommendations to Fix:

1. Increase document context in expert-conversation function
2. Align system prompts for consistency
3. same temperature settings similar output style
4. Consider using extract-answer API for initial generation in conversation flow

Would you like me to implement these fixes to make the automatic answer generation more comprehensive and consistent with the manual  